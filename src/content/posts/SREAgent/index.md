---
title: 'HolmesGPT的设计与实现'
published: 2025-09-07
draft: false
tags: [ 'agent']
toc: true
---


随着大模型的能力越来越强，各种智能助手也在持续涌现，在计算机领域本身，SRE的工作也面临挑战，如何通过AI来优化SRE工作效率，提高系统SLA，也是业界在探索的重点领域之一。本文介绍的HolmesGPT便是其中典型的一款SREAgent

## HolmesGPT 介绍

以A应用页面打开慢为例，从告警通知起始 **步骤2** 至 根因定位 **步骤11** SRE需要登录到各种系统中进行排查、分析、处理。
![SRE故障处理时序.png](./SRE故障处理时序.png)

此时，不难想到一个需求
:::note
能否基于现有的监控系统、环境信息实现根因分析并提供最终解决方案？
:::

__程序员同学，可能会思考：采用运维自动化的方案即可，为何会需要采用大模型？ 在当前AI时代，好问题不怕得不到解答，Gemini Pro 提供了一个很好的答复，[附录：AIOps对比自动化运维](#附录大模型谈aiops对比自动化运维)__

回归HolmesGPT，该作者正是通过以LLM构建的大脑和神经系统以及以可观测、云基础设施、IT服务化流程构建的骨骼和肌肉打造的AI运维助手。

## HolmesGPT 系统架构

HolmesGPT的系统架构设计为帮助SRE处理核心工作 —— **事件响应**，如故障处理、业务诉求反馈、系统巡检等，核心架构如下

1. 事件源头（两种形式），一种为系统触发，比如告警、定时任务、上游系统等等，另一种以自然语言触发，如研发诉求、用户反馈，即HolmesGPT的输入源头；
2. 事件上下文，依赖两类信息
    - 此时此刻：系统现状（所处环境信息及系统自身信息），通过API访问直接与系统交互获取；
    - 彼时彼刻：可观测信息（监控、日志、APM、事件），通过可观测系统保留的事件历史信息快照，记录了事件发生时的状态；
3. 聪明的“大脑”替代SRE工程师，封装从商业到开源大模型，实现自主规划、调用工具、并根据环境反馈来动态调整的“思考”能力；
4. 事件处理SOP，通过Confluence/Internet提供事件处理经验；
5. 事件输出，综合1-4最终形成RCA并输出最终结论。

![holmesgpt](./holmesgpt.png)


## 技术架构及核心技术点

### 技术架构

### 提示词工程

### 可扩展的接口能力

### 更安全的Tool Use

## 展望


## 参考资料

* [什么是AIOps](https://info.support.huawei.com/info-finder/encyclopedia/zh/AIOps.html)
* [什么是可观测性2.0](https://greptime.cn/blogs/2025-04-24-observability2.0-greptimedb.html)


## 附录：AIOps对比自动化运维

:::note
以下内容对自动化运维代码与AI大模型在处理运维问题上的差异和协同关系进行了详细分析和探讨。
:::

这里的关键区别不在于"能"与"不能"，而在于处理问题的**复杂度边界**和**适应性**。自动化代码和 AI 大模型在这里扮演的角色，可以类比为一位严格执行手册的**初级工程师**和一位经验丰富、能触类旁通的**高级专家**。

### 自动化运维代码：处理"已知的未知"

您的自动化代码，本质上是一系列**基于规则的确定性系统**。它的逻辑是：

> **IF** `redis_hit_rate` < 50% **AND** `db_connections` > 1000 **THEN** `ALERT("疑似缓存穿透")` **AND** `EXECUTE("limit_api_rate.sh")`

这套系统极其高效、可靠，前提是：
1.  **你已经预知了这种故障模式**。
2.  **你已经为它编写了明确的规则和预案 (Playbook)**。

它非常擅长处理我们称之为 **"已知的未知"（Known Unknowns）** 的问题——我们知道可能会发生缓存穿透，只是不知道它何时发生。

**局限性在于：**
当出现一个**全新的、从未见过的故障组合**时，这套规则系统就失灵了。比如，某个 Kubernetes CNI 插件的 Bug，导致特定 Node 上的 Pod 网络出现微小的丢包，进而使 Trace 系统的数据上报不完整，最终体现为 APM 图表上出现无法解释的延迟毛刺。

您的自动化脚本里没有针对"CNI 丢包"和"Trace 数据不完整"的 `IF-ELSE`，它就无法将这两者与"延迟毛刺"关联起来，也就无法定位根因。

---

### AI 大模型：处理"未知的未知"

AI（特别是现代的机器学习和 LLM）是一个**基于概率和模式的学习系统**。它不依赖于人类预先编写的硬编码规则。

#### 1. 传统机器学习 (ML) 的价值：发现隐藏的关联

回到我之前提到的第一点"智能根因分析"。这部分其实更多是传统机器学习（时序分析、关联算法）的功劳。
*   **处理高维数据：** 一个现代应用有成千上万个指标。当故障发生时，可能有几十个指标同时异常。人类或简单的规则很难在这么多维度中快速找到根因。ML 算法可以瞬间分析所有指标的动态，计算它们的相关性，找出最可能是"第一张倒下的多米诺骨牌"的那个指标。
*   **动态基线：** 您的规则可能是 `IF P95 > 500ms`。但对于一个深夜低峰期的服务，`P95 = 200ms` 可能就已经是严重异常。AI 可以为每个指标学习其在不同时间（工作日、周末、大促）的"正常"动态范围，从而更精准地发现异常。

#### 2. 大语言模型 (LLM) 的独特价值：理解非结构化数据和推理

这是"为什么一定要用 AI **大模型**"这个问题的核心答案。LLM 带来了传统 ML 算法不具备的能力：

*   **理解自然语言：** 您的 SOP 中有"用户反馈"。LLM 可以实时读取工单、Slack/Teams 里的对话，**理解用户抱怨的"慢"、"卡"、"转圈圈"**，并将其与 APM 监控里的`LCP 指标升高`关联起来。这是自动化脚本无法做到的。
*   **解析日志和变更：** LLM 能读懂 Git 的 commit message、K8s 的变更 YAML 文件、以及应用打印出的非结构化日志。当故障发生时，它可以快速将时间点与这些"人类可读"的变更信息进行关联，直接告诉你：**"10:05 的延迟升高，与 9:50 的一个关于缓存策略的发布（commit #abc123）在时间上高度吻合。"**
*   **生成人类可读的报告：** 在找到根因后，LLM 可以自动生成一份像您 SOP 里那样的、清晰的故障摘要报告，发到 War Room 里，极大提升信息同步效率。

---

### 总结：协同工作的关系

把它们结合起来看：

*   **自动化运维代码：** 是 AIOps 的**骨骼和肌肉**，负责高效地执行明确的指令（扩容、回滚、重启）。
*   **AI 大模型：** 是 AIOps 的**大脑和神经系统**，负责感知（监控数据、日志、用户反馈）、分析（关联、定位）和决策（选择哪个预案）。

**AI 负责分析和决策，输出指令；自动化代码负责接收指令，并可靠地执行。**

所以，您完全不需要抛弃您宝贵的自动化代码。恰恰相反，AI 的加入，能让您的自动化代码变得更加"智能"，让它在面对复杂和未知的场景时，也能被精准地调用，从而实现真正的、有弹性的自动化运维。